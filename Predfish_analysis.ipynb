{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4598bb75",
   "metadata": {},
   "source": [
    "## Step 1: create cultivar specific files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e12fc6f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1_data1.txt', '1_data2.txt']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##split the 0_phenotype.txt into cultivar-specific file\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the file\n",
    "df = pd.read_csv('0_phenotype_data.txt', sep='\\t')\n",
    "\n",
    "# Splitting the file\n",
    "output_files = []\n",
    "for i in range(1, len(df.columns)):\n",
    "    # Selecting ID column and the ith column\n",
    "    col_name = df.columns[i]\n",
    "    subset_df = df.iloc[:, [0, i]]\n",
    "    \n",
    "    # Saving to a new file\n",
    "    output_file_name = f'1_data{i}.txt'\n",
    "    subset_df.to_csv(output_file_name, sep='\\t', index=False)\n",
    "    output_files.append(output_file_name)\n",
    "\n",
    "output_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc9d7a5",
   "metadata": {},
   "source": [
    "## Step 2: change second col header to disease [which was cultivars name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c222a691",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Using glob to find all files starting with '1_data' in the specified directory\n",
    "file_paths = glob.glob('1_data*.txt')\n",
    "\n",
    "# Renaming the second column in each file\n",
    "for file_path in file_paths:\n",
    "    df = pd.read_csv(file_path, sep='\\t')\n",
    "    df.columns = ['ID', 'disease']\n",
    "    \n",
    "    # Save the modified file\n",
    "    df.to_csv(file_path, sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e84bac",
   "metadata": {},
   "source": [
    "## Step 3: calculate median and assign disease to low if value lower than median, and high - if value higher than median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9bc676ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Using glob to find all files starting with '1_data' in the specified directory\n",
    "data_files = glob.glob('1_data*.txt')\n",
    "\n",
    "# Processing each file\n",
    "for data_file in data_files:\n",
    "    data_df = pd.read_csv(data_file, sep='\\t')\n",
    "\n",
    "    # Check if 'disease' column exists\n",
    "\n",
    "    \n",
    "    if 'disease' in data_df.columns:\n",
    "        # Calculate the median\n",
    "        median_value = data_df['disease'].median()\n",
    "\n",
    "        # Replace values based on the median\n",
    "        data_df['disease'] = data_df['disease'].apply(lambda x: 'low' if x < median_value else 'high')\n",
    "\n",
    "        # Save the modified dataframe back to the file\n",
    "        data_df.to_csv(data_file, sep='\\t', index=False)\n",
    "    else:\n",
    "        print(f\"'disease' column not found in {data_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f04c392",
   "metadata": {},
   "source": [
    "## Step 4: process the complete isoform table - remove isoform frequency <5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "634f9be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##check number of isoform in bash\n",
    "#awk -F'\\t' '{print NF; exit}' isoform_output_combined_zymo.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "98c27d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "## removeing all the isoform with freq <5\n",
    "# this step can be resourse intensive, so better in nimbus------------------------\n",
    "import pandas as pd\n",
    "\n",
    "# Load the file into a pandas DataFrame\n",
    "df = pd.read_csv('0_combined_isoform.txt', sep='\\t')\n",
    "\n",
    "# Sum the frequencies of each isoform across all samples\n",
    "isoform_sums = df.iloc[:, 1:].sum()\n",
    "\n",
    "# Filter out isoforms with a sum of 3 or less\n",
    "filtered_isoforms = isoform_sums[isoform_sums > 5]\n",
    "\n",
    "# Creating a new DataFrame with only the filtered isoforms\n",
    "filtered_df = df[['ID'] + list(filtered_isoforms.index)]\n",
    "\n",
    "# Saving the filtered DataFrame to a new file\n",
    "filtered_df.to_csv('0_filtered_combined_isoform.txt', index=False, sep='\\t')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b90959",
   "metadata": {},
   "source": [
    "## step 5: merge cultivar-specific file with combine isoform file by ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7ca69940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated file saved: 2_merged_data1.txt\n",
      "Concatenated file saved: 2_merged_data2.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "\n",
    "# File to be concatenated\n",
    "additional_file = '0_filtered_combined_isoform.txt'\n",
    "\n",
    "# Read the additional file\n",
    "additional_df = pd.read_csv(additional_file, sep='\\t')\n",
    "\n",
    "# Using glob to find all files starting with '1_data' in the specified directory\n",
    "data_files = glob.glob( '1_data*.txt')\n",
    "\n",
    "# Concatenating each file with the additional file\n",
    "for data_file in data_files:\n",
    "    # Read the data file\n",
    "    data_df = pd.read_csv(data_file, sep='\\t')\n",
    "\n",
    "    # Concatenate with the additional dataframe on 'ID' column\n",
    "    concatenated_df = pd.merge(data_df, additional_df, on='ID', how='left')\n",
    "\n",
    "    # Corrected extraction of file number\n",
    "    file_number = os.path.basename(data_file).replace('1_data', '').replace('.txt', '')\n",
    "    \n",
    "    # Save the concatenated dataframe to a new file\n",
    "    concatenated_file_path =  f'2_merged_data{file_number}.txt'\n",
    "    concatenated_df.to_csv(concatenated_file_path, sep='\\t', index=False)\n",
    "\n",
    "    print(f\"Concatenated file saved: {concatenated_file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3243800",
   "metadata": {},
   "source": [
    "## Step 6: change 0 = A and 1 = P in those files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e924e0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Using glob to find all files starting with '1_data' in the specified directory\n",
    "data_files = glob.glob('2_merged_data*.txt')\n",
    "\n",
    "# Processing each file\n",
    "for data_file in data_files:\n",
    "    data_df = pd.read_csv(data_file, sep='\\t')\n",
    "\n",
    "    # Looping through all columns after the second one\n",
    "    for col in data_df.columns[2:]:\n",
    "        # Replace 1 with 'P' and 0 with 'A'\n",
    "        data_df[col] = data_df[col].apply(lambda x: 'P' if x == 1.0 else ('A' if x == 0.0 else x))\n",
    "\n",
    "    # Save the modified dataframe back to the file\n",
    "    data_df.to_csv(data_file, sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d3e14d",
   "metadata": {},
   "source": [
    "## step 7: contingency table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "66d35772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script completed for dataset 1. The formatted contingency table has been written to '3_contingency_table_data1.txt' and '4_hypergeo_data1.txt'\n",
      "Script completed for dataset 2. The formatted contingency table has been written to '3_contingency_table_data2.txt' and '4_hypergeo_data2.txt'\n"
     ]
    }
   ],
   "source": [
    "###step 2: contingency table \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "for i in range(1, 3):  # Loop from 1 to 12 for your 12 data files----------------------------------neeed to be automated here \n",
    "    # Dynamically create file name\n",
    "    file_name = f'2_merged_data{i}.txt'\n",
    "\n",
    "    # Load data\n",
    "    data = pd.read_csv(file_name, delimiter='\\t')  # Adjust delimiter if needed\n",
    "\n",
    "    # Melt the data\n",
    "    melted = pd.melt(data, id_vars=['disease'], var_name='isoform', value_name='value')\n",
    "\n",
    "    # Pivot the table to get the contingency table\n",
    "    pivot_data = melted.pivot_table(index='isoform', columns=['disease', 'value'], aggfunc='size', fill_value=0)\n",
    "\n",
    "    # Create new headers\n",
    "    pivot_data.columns = ['{}-{}'.format(disease, value) for disease, value in pivot_data.columns]\n",
    "\n",
    "    # Filter columns\n",
    "    desired_columns = ['high-A', 'high-P', 'low-A', 'low-P']\n",
    "    pivot_data = pivot_data[desired_columns]\n",
    "\n",
    "    # Remove the first row (which contains 'ID')\n",
    "    pivot_data = pivot_data.drop(pivot_data.index[0])\n",
    "\n",
    "    # Write the contingency table to a file\n",
    "    contingency_table_file = f'3_contingency_table_data{i}.txt'\n",
    "    pivot_data.to_csv(contingency_table_file, sep='\\t', index=True)\n",
    "\n",
    "    # Rename the columns\n",
    "    column_rename = {'high-A': 'c', 'high-P': 'a', 'low-A': 'd', 'low-P': 'b'}\n",
    "    pivot_data.rename(columns=column_rename, inplace=True)\n",
    "\n",
    "    # Write the reformatted contingency table to a new file\n",
    "    hypergeo_data_file = f'4_hypergeo_data{i}.txt'\n",
    "    pivot_data.to_csv(hypergeo_data_file, sep='\\t', index=True)\n",
    "\n",
    "    print(f\"Script completed for dataset {i}. The formatted contingency table has been written to '{contingency_table_file}' and '{hypergeo_data_file}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4f1e38",
   "metadata": {},
   "source": [
    "## Step 8: hypergeomatric test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c516e516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset number 1...\n",
      "Processing dataset number 2...\n",
      "All datasets processed.\n"
     ]
    }
   ],
   "source": [
    "###step 3 - automated: Hypergeo test - loop over many datasets together [automated]\n",
    "###loop over all dataset\n",
    "import math\n",
    "\n",
    "# Factorial calculation function\n",
    "def calculate_factorials(n):\n",
    "    fact = [0] * (n + 1)\n",
    "    fact[0] = 1\n",
    "    for i in range(2, n + 1):\n",
    "        fact[i] = fact[i - 1] + math.log(i)\n",
    "    return fact\n",
    "\n",
    "def process_dataset(input_file, output_file, fact):\n",
    "    with open(output_file, 'w') as fileout, open(input_file, 'r') as input_data:\n",
    "        fileout.write(\"isoform\\thigh-A\\thigh-P\\tlow-A\\tlow-P\\tp-value\\n\")\n",
    "        next(input_data)  # Skip the header row\n",
    "        for line_num, line in enumerate(input_data, 2):\n",
    "            tabdata = line.strip().split('\\t')\n",
    "            try:\n",
    "                iso = tabdata[0]\n",
    "                a = round(float(tabdata[1]))\n",
    "                c = round(float(tabdata[2]))\n",
    "                b = round(float(tabdata[3]))\n",
    "                d = round(float(tabdata[4]))\n",
    "            except ValueError as e:\n",
    "                print(f\"Error on line {line_num}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            n = a + b + c + d\n",
    "            hyp1 = fact[a + b] + fact[c + d] + fact[a + c] + fact[b + d] - (fact[n] + fact[a] + fact[b] + fact[c] + fact[d])\n",
    "            hyp1 = math.exp(hyp1)\n",
    "            fileout.write(f\"{iso}\\t{a}\\t{c}\\t{b}\\t{d}\\t{hyp1}\\n\")\n",
    "\n",
    "# Determine the range of your datasets\n",
    "start_dataset_number = 1  # start number -------------------------------------------------------need to change here (need to be automated)\n",
    "end_dataset_number = 2   # end number ---------------------------------------------------------need to change here (need to be automated)\n",
    "\n",
    "# Pre-calculate factorials\n",
    "fact = calculate_factorials(1000000)\n",
    "\n",
    "# Loop over the datasets\n",
    "for i in range(start_dataset_number, end_dataset_number + 1):\n",
    "    input_file = f'4_hypergeo_data{i}.txt'\n",
    "    output_file = f'5_output_hypergeo_data{i}.txt'\n",
    "    print(f\"Processing dataset number {i}...\")\n",
    "    process_dataset(input_file, output_file, fact)\n",
    "\n",
    "print(\"All datasets processed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4147efa4",
   "metadata": {},
   "source": [
    "## Step 9: merge all the cultivar-specific p-value (master data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "31963aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script completed. Merged data written to 'merged_output_hypergeo_data.txt'\n"
     ]
    }
   ],
   "source": [
    "###step 4: merge_pvalue - cultivar specific [automated]\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Specify the pattern for your files\n",
    "file_pattern = '5_output_hypergeo_data*.txt'\n",
    "files = glob.glob(file_pattern)\n",
    "\n",
    "# Initialize an empty DataFrame for the merged data\n",
    "merged_data = pd.DataFrame()\n",
    "\n",
    "# Process each file\n",
    "for file_number, file_path in enumerate(sorted(files), start=1):\n",
    "    # Read the file, focusing only on the 'isoform' and 'p-value' columns\n",
    "    data = pd.read_csv(file_path, delimiter='\\t', usecols=['isoform', 'p-value'])\n",
    "    \n",
    "    # Rename the 'p-value' column to 'p-value-{i}'\n",
    "    p_value_column = f'p-value-{file_number}'\n",
    "    data.rename(columns={'p-value': p_value_column}, inplace=True)\n",
    "    \n",
    "    # If merged_data is empty, initialize it with the data from the first file\n",
    "    if merged_data.empty:\n",
    "        merged_data = data\n",
    "    else:\n",
    "        # Merge the current data with the merged_data on 'isoform'\n",
    "        merged_data = pd.merge(merged_data, data, on='isoform', how='outer')\n",
    "\n",
    "# Save the merged data to a new file\n",
    "merged_data.to_csv('6_merged_p-value.txt', sep='\\t', index=False)\n",
    "\n",
    "print(\"Script completed. Merged data written to 'merged_output_hypergeo_data.txt'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07326676",
   "metadata": {},
   "source": [
    "## Step 10: calculate lowest p-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "11cbc2d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7_merged_lowest_p-value.txt'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-importing pandas and redefining the code after the reset\n",
    "import pandas as pd\n",
    "\n",
    "# File path\n",
    "file_path = '6_merged_p-value.txt'\n",
    "\n",
    "# Reading the file\n",
    "df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "# Finding columns that start with 'p-value'\n",
    "p_value_cols = [col for col in df.columns if col.startswith('p-value')]\n",
    "\n",
    "# Calculating the lowest p-value and storing it in a new column\n",
    "df[\"p-value_lowest\"] = df[p_value_cols].min(axis=1)\n",
    "\n",
    "# Saving the modified dataframe\n",
    "modified_file_path = '7_merged_lowest_p-value.txt'\n",
    "df.to_csv(modified_file_path, sep='\\t', index=False)\n",
    "\n",
    "modified_file_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06ed479",
   "metadata": {},
   "source": [
    "## Step 11: need to create col locus_id from isoform_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a633bf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###preparing cultivar specific fisher p-value for combining\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read the merged output\n",
    "merged_df = pd.read_csv(\"7_merged_lowest_p-value.txt\", delimiter=\"\\t\")\n",
    "\n",
    "# Duplicate \"isoform_id\" column for further processing\n",
    "merged_df[\"locus_id\"] = merged_df[\"isoform\"]\n",
    "\n",
    "# Use regex to process the \"variable\" column\n",
    "merged_df[\"locus_id\"] = merged_df[\"locus_id\"].str.replace(r'(_\\d+)$', '', regex=True)\n",
    "\n",
    "# Use case-insensitive regex to remove A, B, C, D, or any combination thereof from the end\n",
    "merged_df[\"locus_id\"] = merged_df[\"locus_id\"].str.replace(r'(?i)([A-D]+)$', '', regex=True)\n",
    "\n",
    "# Move \"variable\" column to the first position\n",
    "cols = ['locus_id'] + [col for col in merged_df if col != 'locus_id']\n",
    "merged_df = merged_df[cols]\n",
    "\n",
    "# Save the updated dataframe\n",
    "merged_df.to_csv(\"8_merged_lowest_p-value_with_locus_id.txt\", index=False, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da364915",
   "metadata": {},
   "source": [
    "## Step 12: combine fisher result with predector result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "afadd742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging complete!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data files\n",
    "df1 = pd.read_csv(\"8_merged_lowest_p-value_with_locus_id.txt\", sep=\"\\t\")\n",
    "df2 = pd.read_csv(\"0_predector_results.txt\", sep=\"\\t\")\n",
    "\n",
    "# Ensure that 'locus_id' is the first column in both dataframes\n",
    "df1 = df1[['locus_id'] + [col for col in df1.columns if col != 'locus_id']]\n",
    "df2 = df2[['locus_id'] + [col for col in df2.columns if col != 'locus_id']]\n",
    "\n",
    "# Merge the dataframes on 'locus_id'\n",
    "merged_df = df1.merge(df2, on=\"locus_id\", how=\"left\")\n",
    "\n",
    "# Fill NA values if 'locus_id' from df1 is not found in df2\n",
    "merged_df.fillna('NA', inplace=True)\n",
    "\n",
    "# Save the merged data to a new file\n",
    "merged_df.to_csv(\"10_pred_fisher_merged_dataset.txt\", index=False, sep=\"\\t\")\n",
    "\n",
    "print(\"Merging complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20e5813",
   "metadata": {},
   "source": [
    "## Step 13: calculate predfish value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a4339e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New columns have been added.\n"
     ]
    }
   ],
   "source": [
    "# Predector - fisher caculation nd filtering \n",
    "import csv\n",
    "\n",
    "# Lists to store the processed data\n",
    "new_data = []\n",
    "\n",
    "# Read the content of the original TSV file\n",
    "with open(\"10_pred_fisher_merged_dataset.txt\", \"r\") as file: #####change data set here############\n",
    "    reader = csv.reader(file, delimiter=\"\\t\")\n",
    "    \n",
    "    # Process header (add new column name)\n",
    "    header = next(reader)\n",
    "    header.extend([\"convt_p-value\",\"nor_pred_score\", \"PF_score\", \"known_effector\"])\n",
    "    new_data.append(header)\n",
    "\n",
    "    # Index of the p-value column\n",
    "    p_value_idx = header.index(\"p-value_lowest\")\n",
    "    pred_score_idx = header.index(\"effector_score\")\n",
    "    variable_idx = header.index(\"locus_id\")\n",
    "    cyst_idx = header.index(\"aa_c_number\")\n",
    "    total_aa_idx = header.index(\"residue_number\")\n",
    "\n",
    "    # Process data rows\n",
    "    for row in reader:\n",
    "\n",
    "        try:\n",
    "        #    # Check the conditions for filtering row\n",
    "           #if float(row[cyst_idx]) < 2 or float(row[total_aa_idx]) > 300  or float(row[pred_score_idx]) < 2 or float(row[p_value_idx]) > 0.05: ###------------------need to change the filters here\n",
    "           if float(row[pred_score_idx]) < 2 :\n",
    "            #if float(row[total_aa_idx]) > 3000:\n",
    "                continue  # Skip the row if any condition is met\n",
    "        except ValueError:  # handle non-numeric values # this is need as there are some NA value, that why the following section did not work\n",
    "            continue\n",
    "\n",
    "        # Calculate convt_p-value\n",
    "        convt_p_value = 1 - float(row[p_value_idx])\n",
    "    \n",
    "        # Calculate nor_pred_score\n",
    "        nor_pred_score = (float(row[pred_score_idx]) - 1.001) / (3.941 - 1.001)\n",
    "    \n",
    "        # Calculate PF_score\n",
    "        PF_score = (nor_pred_score * 0.5) + (convt_p_value * 0.5)\n",
    "        \n",
    "        # Determine known_effector based on variable column\n",
    " # Determine known_effector based on variable column\n",
    "        if \"SNOO_200780\" in row[variable_idx]:\n",
    "            known_effector = \"Tox1\"\n",
    "        elif \"SNOO_165710\" in row[variable_idx]:\n",
    "            known_effector = \"ToxA\"\n",
    "        elif \"SNOO_423420\" in row[variable_idx]:\n",
    "            known_effector = \"Tox1-like\"\n",
    "        elif \"SNOO_089810\" in row[variable_idx]:\n",
    "            known_effector = \"Tox3\"\n",
    "        elif \"SNOO_144930\" in row[variable_idx]:\n",
    "            known_effector = \"Tox267\"\n",
    "        elif \"SNOO_060150\" in row[variable_idx]:\n",
    "            known_effector = \"Tox8\"\n",
    "        elif \"SNOO_503200\" in row[variable_idx]:\n",
    "            known_effector = \"Tox5\"\n",
    "        elif \"SNOO_529790\" in row[variable_idx]:\n",
    "            known_effector = \"homologue - tox1, tox1-like\"\n",
    "        elif \"SNOO_083550\" in row[variable_idx]:\n",
    "            known_effector = \"homologue - tox1\"\n",
    "        elif \"SNOO_010970\" in row[variable_idx]:\n",
    "            known_effector = \"homologue - tox3, tox5, tox267\"\n",
    "        elif \"SNOO_064590\" in row[variable_idx]:\n",
    "            known_effector = \"Hydrophobin2-MHP1]\"\n",
    "        elif \"SNOO_031220\" in row[variable_idx]:\n",
    "            known_effector = \"Hydrophobin2-MHP1]\"\n",
    "        elif \"SNOO_017620\" in row[variable_idx]:\n",
    "            known_effector = \"TUB1\"\n",
    "        elif \"SNOO_045580\" in row[variable_idx]:\n",
    "            known_effector = \"Actin-cap_B\"\n",
    "        elif \"SNOO_011390\" in row[variable_idx]:\n",
    "            known_effector = \"Actin-ARP1\"\n",
    "        elif \"SNOO_012780\" in row[variable_idx]:\n",
    "            known_effector = \"GAPDH\"\n",
    "        else:\n",
    "            known_effector = \"\"\n",
    "        \n",
    "        # Append new columns to the row\n",
    "        row.extend([str(convt_p_value), str(nor_pred_score), str(PF_score), known_effector])\n",
    "        \n",
    "        new_data.append(row)\n",
    "\n",
    "# Write the processed data to the same file\n",
    "with open(\"11_pred_fisher_test_result.txt\", \"w\", newline='') as outfile: #####change data set here############\n",
    "    writer = csv.writer(outfile, delimiter=\"\\t\")\n",
    "    writer.writerows(new_data)\n",
    "\n",
    "print(\"New columns have been added.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a5ee61",
   "metadata": {},
   "source": [
    "## Step 14: final locus list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "05671f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data into a pandas DataFrame\n",
    "data = pd.read_csv(\"11_pred_fisher_test_result.txt\", sep=\"\\t\") #####change data set here############\n",
    "\n",
    "# Remove rows with duplicate values in 'variable' column, while keeping the first occurrence\n",
    "data_no_duplicates = data.drop_duplicates(subset='locus_id', keep='first')\n",
    "\n",
    "# Save the modified DataFrames back to files\n",
    "data_no_duplicates.to_csv('12_final_locus_data_pf_only_predscore_2.txt', sep='\\t', index=False) #####change data set here############\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca59cae0",
   "metadata": {},
   "source": [
    "## extra\n",
    "this is for ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "80336a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###ranking based on three methods\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data into a pandas DataFrame\n",
    "data = pd.read_csv(\"11_pred_fisher_test_result.txt\", sep=\"\\t\") #####change data set here############\n",
    "\n",
    "# Remove rows with duplicate values in 'variable' column, while keeping the first occurrence\n",
    "data_no_duplicates = data.drop_duplicates(subset='locus_id', keep='first')\n",
    "\n",
    "###---------------------------------------------------\n",
    "# Sort the data based on PF_score\n",
    "sorted_data = data_no_duplicates.sort_values(by=\"PF_score\", ascending=False)\n",
    "\n",
    "# Find and populate the rows where \"known_effector\" column is not empty\n",
    "effectors_table = [[\"Ranking\", \"Known_effector\", \"p-value_lowest\", \"effector_score\", \"PF_score\"]]\n",
    "for i, (_, row) in enumerate(sorted_data.iterrows(), start=1):  # Using iterrows to iterate over DataFrame rows\n",
    "    if pd.notnull(row[\"known_effector\"]):  # Using pandas' notnull to check for non-empty values\n",
    "        effectors_table.append([i, row[\"known_effector\"], row[\"p-value_lowest\"], row[\"effector_score\"],row[\"PF_score\"]])\n",
    "\n",
    "# Convert the effectors_table list to a DataFrame\n",
    "effectors_df = pd.DataFrame(effectors_table[1:], columns=effectors_table[0])\n",
    "\n",
    "# Save the modified DataFrames back to files\n",
    "data_no_duplicates.to_csv('12_final_locus_data_pf.txt', sep='\\t', index=False) #####change data set here############\n",
    "effectors_df.to_csv('13_ranking_PF.txt', sep='\\t', index=False) #####change data set here############\n",
    "\n",
    "###---------------------------------------------------\n",
    "# Sort the data based on PF_score\n",
    "sorted_data = data_no_duplicates.sort_values(by=\"effector_score\", ascending=False)\n",
    "\n",
    "# Find and populate the rows where \"known_effector\" column is not empty\n",
    "effectors_table = [[\"Ranking\", \"Known_effector\", \"p-value_lowest\", \"effector_score\", \"PF_score\"]]\n",
    "for i, (_, row) in enumerate(sorted_data.iterrows(), start=1):  # Using iterrows to iterate over DataFrame rows\n",
    "    if pd.notnull(row[\"known_effector\"]):  # Using pandas' notnull to check for non-empty values\n",
    "        effectors_table.append([i, row[\"known_effector\"], row[\"p-value_lowest\"], row[\"effector_score\"],row[\"PF_score\"]])\n",
    "\n",
    "# Convert the effectors_table list to a DataFrame\n",
    "effectors_df = pd.DataFrame(effectors_table[1:], columns=effectors_table[0])\n",
    "\n",
    "# Save the modified DataFrames back to files\n",
    "#data_no_duplicates.to_csv('12_final_locus_data_pf.txt', sep='\\t', index=False) #####change data set here############\n",
    "effectors_df.to_csv('13_ranking_pred_score.txt', sep='\\t', index=False) #####change data set here############\n",
    "\n",
    "###---------------------------------------------------\n",
    "# Sort the data based on PF_score\n",
    "sorted_data = data_no_duplicates.sort_values(by=\"p-value_lowest\", ascending=False)\n",
    "\n",
    "# Find and populate the rows where \"known_effector\" column is not empty\n",
    "effectors_table = [[\"Ranking\", \"Known_effector\", \"p-value_lowest\", \"effector_score\", \"PF_score\"]]\n",
    "for i, (_, row) in enumerate(sorted_data.iterrows(), start=1):  # Using iterrows to iterate over DataFrame rows\n",
    "    if pd.notnull(row[\"known_effector\"]):  # Using pandas' notnull to check for non-empty values\n",
    "        effectors_table.append([i, row[\"known_effector\"], row[\"p-value_lowest\"], row[\"effector_score\"],row[\"PF_score\"]])\n",
    "\n",
    "# Convert the effectors_table list to a DataFrame\n",
    "effectors_df = pd.DataFrame(effectors_table[1:], columns=effectors_table[0])\n",
    "\n",
    "# Save the modified DataFrames back to files\n",
    "data_no_duplicates.to_csv('12_final_locus_data_p-value.txt', sep='\\t', index=False) #####change data set here############\n",
    "effectors_df.to_csv('13_ranking_p-value.txt', sep='\\t', index=False) #####change data set here############\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdec9e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
